#!/usr/bin/env python3
"""
æµ‹è¯•è°ƒè¯•æ—¥å¿—åŠŸèƒ½çš„è„šæœ¬

æ­¤è„šæœ¬å‘é€ä¸€ä¸ªæµ‹è¯•è¯·æ±‚åˆ°åä»£æœåŠ¡ï¼Œä»¥éªŒè¯æ˜¯å¦èƒ½æ­£ç¡®è®°å½•ï¼š
1. ç›®æ ‡APIçš„åŸå§‹å“åº”å†…å®¹
2. æ­£åˆ™å¤„ç†åçš„å“åº”å†…å®¹
"""

import asyncio
import json
import aiohttp

async def test_debug_logs():
    """æµ‹è¯•è°ƒè¯•æ—¥å¿—åŠŸèƒ½"""
    
    # æµ‹è¯•ç”¨çš„OpenAI APIè¯·æ±‚
    test_request = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {
                "role": "user", 
                "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚"
            }
        ],
        "stream": False  # å…ˆæµ‹è¯•éæµå¼
    }
    
    # å‡è®¾æœåŠ¡å™¨è¿è¡Œåœ¨ localhost:8000
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªæµ‹è¯•ç”¨çš„ç›®æ ‡URLï¼ˆå®é™…æµ‹è¯•æ—¶éœ€è¦æ›¿æ¢ä¸ºæœ‰æ•ˆçš„OpenAIå…¼å®¹APIï¼‰
    proxy_url = "http://localhost:8000/https://api.openai.com/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-test-key-here"  # éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥
    }
    
    print("ğŸ“‹ æµ‹è¯•è°ƒè¯•æ—¥å¿—åŠŸèƒ½...")
    print(f"ğŸ¯ ç›®æ ‡: {proxy_url}")
    print(f"ğŸ“¨ è¯·æ±‚ä½“: {json.dumps(test_request, ensure_ascii=False, indent=2)}")
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                proxy_url,
                json=test_request,
                headers=headers
            ) as response:
                print(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status}")
                
                if response.status == 200:
                    response_data = await response.json()
                    print("âœ… è¯·æ±‚æˆåŠŸï¼")
                    print(f"ğŸ“„ å“åº”å†…å®¹: {json.dumps(response_data, ensure_ascii=False, indent=2)}")
                    print("\nğŸ” è¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—ä¸­çš„è°ƒè¯•ä¿¡æ¯ï¼š")
                    print("   - 'è¯·æ±‚ä½“' æ—¥å¿—ï¼ˆæ¢è¡ŒJSONæ ¼å¼ï¼‰")
                    print("   - 'ç›®æ ‡APIåŸå§‹å“åº”å†…å®¹' æ—¥å¿—")
                    print("   - 'æ­£åˆ™å¤„ç†åçš„å“åº”å†…å®¹' æ—¥å¿—")
                else:
                    error_text = await response.text()
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {error_text}")
                    
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿åä»£æœåŠ¡æ­£åœ¨è¿è¡Œåœ¨ localhost:8000")

async def test_debug_logs_streaming():
    """æµ‹è¯•æµå¼è¯·æ±‚çš„è°ƒè¯•æ—¥å¿—åŠŸèƒ½"""
    
    # æµ‹è¯•ç”¨çš„æµå¼è¯·æ±‚
    test_request = {
        "model": "gpt-3.5-turbo", 
        "messages": [
            {
                "role": "user",
                "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ ã€‚"
            }
        ],
        "stream": True  # æµå¼è¯·æ±‚
    }
    
    proxy_url = "http://localhost:8000/https://api.openai.com/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-test-key-here"  # éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥
    }
    
    print("\nğŸ“‹ æµ‹è¯•æµå¼è¯·æ±‚è°ƒè¯•æ—¥å¿—åŠŸèƒ½...")
    print(f"ğŸ¯ ç›®æ ‡: {proxy_url}")
    print(f"ğŸ“¨ æµå¼è¯·æ±‚ä½“: {json.dumps(test_request, ensure_ascii=False, indent=2)}")
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                proxy_url,
                json=test_request,
                headers=headers
            ) as response:
                print(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status}")
                
                if response.status == 200:
                    print("âœ… æµå¼è¯·æ±‚æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
                    
                    chunk_count = 0
                    async for chunk in response.content.iter_chunked(1024):
                        if chunk:
                            chunk_count += 1
                            print(f"ğŸ“¦ æ¥æ”¶åˆ°æ•°æ®å— {chunk_count}: {len(chunk)} å­—èŠ‚")
                            
                            # åªæ˜¾ç¤ºå‰å‡ ä¸ªæ•°æ®å—çš„å†…å®¹
                            if chunk_count <= 3:
                                try:
                                    chunk_text = chunk.decode('utf-8')
                                    print(f"   å†…å®¹é¢„è§ˆ: {chunk_text[:100]}...")
                                except:
                                    print(f"   äºŒè¿›åˆ¶å†…å®¹")
                    
                    print(f"âœ… æµå¼è¯·æ±‚å®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
                    print("\nğŸ” è¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—ä¸­çš„æµå¼è°ƒè¯•ä¿¡æ¯ï¼š")
                    print("   - 'æµå¼è¯·æ±‚ä½“' æ—¥å¿—ï¼ˆæ¢è¡ŒJSONæ ¼å¼ï¼‰")
                    print("   - 'æµå¼å“åº”å®Œæ•´åŸå§‹å†…å®¹' æ—¥å¿—")
                    print("   - 'æµå¼å“åº”æ­£åˆ™å¤„ç†åå†…å®¹' æ—¥å¿—")
                    print("   - 'æ¨¡æ‹Ÿæµå¼å“åº”æ­£åˆ™å¤„ç†åå†…å®¹' æ—¥å¿—ï¼ˆå¦‚æœå¯ç”¨äº†fake streamingï¼‰")
                else:
                    error_text = await response.text()
                    print(f"âŒ æµå¼è¯·æ±‚å¤±è´¥: {error_text}")
                    
    except Exception as e:
        print(f"âŒ æµå¼è¿æ¥å¤±è´¥: {e}")

if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹æµ‹è¯•è°ƒè¯•æ—¥å¿—åŠŸèƒ½...")
    print("=" * 60)
    
    # è¿è¡Œéæµå¼æµ‹è¯•
    asyncio.run(test_debug_logs())
    
    # ç­‰å¾…ä¸€ä¸‹å†è¿è¡Œæµå¼æµ‹è¯•
    print("\n" + "=" * 60)
    asyncio.run(test_debug_logs_streaming())
    
    print("\n" + "=" * 60)
    print("ğŸ æµ‹è¯•å®Œæˆï¼")
    print("ğŸ’¡ æç¤ºï¼š")
    print("   1. ç¡®ä¿åœ¨ config/settings.yaml ä¸­è®¾ç½® log_level: 'DEBUG'")
    print("   2. æ£€æŸ¥æœåŠ¡å™¨æ§åˆ¶å°è¾“å‡ºä¸­çš„è°ƒè¯•æ—¥å¿—")
    print("   3. è°ƒè¯•æ—¥å¿—ä¼šæ˜¾ç¤ºå®Œæ•´çš„å“åº”å†…å®¹å’Œæ­£åˆ™å¤„ç†ç»“æœ") 